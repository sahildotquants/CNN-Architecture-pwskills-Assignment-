{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Architecture**"
      ],
      "metadata": {
        "id": "cKgLsNbFVs7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1: What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?**\n"
      ],
      "metadata": {
        "id": "G8jbHCKtV07q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "In a CNN, filters (also called kernels) slide over the image and detect specific patterns like edges, textures, or colors. Each filter learns a different pattern during training.\n",
        "\n",
        "The output created by applying a filter is called a feature map. It shows where that specific pattern was found in the image. As you go deeper into the network, feature maps capture more complex patterns like shapes or objects.\n",
        "\n",
        "So, filters detect patterns, and feature maps represent those patterns in the image."
      ],
      "metadata": {
        "id": "SEAaMatbV8uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?"
      ],
      "metadata": {
        "id": "nZ0qC15_WZxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "Padding means adding extra pixels (usually zeros) around the image before applying a filter.\n",
        "Its purpose is to control how much the image shrinks after convolution.\n",
        "\n",
        "\n",
        " - With padding, the output feature map stays larger.\n",
        "\n",
        "\n",
        " - Without padding, the feature map becomes smaller.\n",
        "\n",
        "\n",
        " - Stride is how many pixels the filter moves at a time.\n",
        "\n",
        "\n",
        " - A stride of 1 moves the filter one pixel at a time, producing a larger output.\n",
        "\n",
        "\n",
        " - A stride greater than 1 skips pixels, producing a smaller output.\n",
        "\n",
        "\n",
        " - Effect:\n",
        "\n",
        "        Padding increases output size, while a larger stride decreases output size."
      ],
      "metadata": {
        "id": "YDtnF5XDWfVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?**\n"
      ],
      "metadata": {
        "id": "Sb8RP91kW_ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The receptive field is the region of the input image that a particular neuron in a CNN \"looks at\" or responds to.\n",
        "\n",
        "In shallow layers, the receptive field is small because neurons only see tiny parts of the image. As the network becomes deeper, the receptive field grows because each layer builds on the previous layer’s outputs.\n",
        "\n",
        "Importance:\n",
        "A larger receptive field allows deep CNNs to understand bigger and more complex patterns, such as shapes, objects, or context in the image. This helps the network make more accurate predictions."
      ],
      "metadata": {
        "id": "lSn2JtAYXJhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4: Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.**\n"
      ],
      "metadata": {
        "id": "3zQAGUwxXi9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "The filter size affects the number of parameters because each filter has its own weights.\n",
        "A larger filter (for example 5×5 instead of 3×3) means more weights, so the CNN has more parameters.\n",
        "\n",
        "The stride does not change the number of parameters in the filters.\n",
        "It only controls how the filter moves across the image.\n",
        "Changing stride affects the output size, not the number of learnable weights.\n",
        "\n",
        "So, filter size increases parameters, while stride does not affect parameters."
      ],
      "metadata": {
        "id": "j3Ky6xGlXnzd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.**\n"
      ],
      "metadata": {
        "id": "oqKkWpTnXwrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        " - **LeNet** is one of the earliest CNNs. It is shallow, with only a few convolutional layers. It uses small filters and works well for simple tasks like digit recognition. Its performance is basic compared to modern models.\n",
        "\n",
        " - **AlexNet**is deeper than LeNet and introduced larger filters in the early layers. It uses ReLU activation and dropout. It performs much better than LeNet and was a major breakthrough for large-scale image classification.\n",
        "\n",
        " - **VGG** is much deeper than both LeNet and AlexNet. It uses many layers with small 3×3 filters. Its depth gives very strong performance and high accuracy, but it requires more computation and memory.\n",
        "\n",
        "In summary: LeNet is shallow, AlexNet is deeper with larger filters, and VGG is the deepest with small filters but very high performance."
      ],
      "metadata": {
        "id": "pV8XrnczYH6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation.**"
      ],
      "metadata": {
        "id": "wwDw9rSKYZLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape and normalize\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Evaluate\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Accuracy:\", test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWGGbkLUY0ij",
        "outputId": "6b8a38ec-4efc-4b32-ccec-9ecf055e2985"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 54ms/step - accuracy: 0.8832 - loss: 0.3867 - val_accuracy: 0.9865 - val_loss: 0.0498\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 54ms/step - accuracy: 0.9836 - loss: 0.0516 - val_accuracy: 0.9863 - val_loss: 0.0440\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 52ms/step - accuracy: 0.9894 - loss: 0.0328 - val_accuracy: 0.9900 - val_loss: 0.0326\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 54ms/step - accuracy: 0.9928 - loss: 0.0211 - val_accuracy: 0.9905 - val_loss: 0.0356\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 53ms/step - accuracy: 0.9947 - loss: 0.0170 - val_accuracy: 0.9917 - val_loss: 0.0341\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9854 - loss: 0.0436\n",
            "Test Accuracy: 0.9887999892234802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture.**\n"
      ],
      "metadata": {
        "id": "NTHksHxBaX27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values (RGB)\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Conv2D(128, (3,3), activation='relu'),\n",
        "    MaxPooling2D((2,2)),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuoTDo4nadCL",
        "outputId": "34dfef10-022d-4281-bc65-6496e7295ca5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Epoch 1/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 93ms/step - accuracy: 0.3153 - loss: 1.8594 - val_accuracy: 0.5242 - val_loss: 1.3119\n",
            "Epoch 2/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 89ms/step - accuracy: 0.5574 - loss: 1.2451 - val_accuracy: 0.6140 - val_loss: 1.1130\n",
            "Epoch 3/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 86ms/step - accuracy: 0.6300 - loss: 1.0511 - val_accuracy: 0.6424 - val_loss: 1.0215\n",
            "Epoch 4/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 88ms/step - accuracy: 0.6787 - loss: 0.9164 - val_accuracy: 0.6730 - val_loss: 0.9639\n",
            "Epoch 5/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - accuracy: 0.7069 - loss: 0.8386 - val_accuracy: 0.7016 - val_loss: 0.8836\n",
            "Epoch 6/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 88ms/step - accuracy: 0.7366 - loss: 0.7585 - val_accuracy: 0.7082 - val_loss: 0.8657\n",
            "Epoch 7/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 90ms/step - accuracy: 0.7550 - loss: 0.6957 - val_accuracy: 0.7066 - val_loss: 0.8831\n",
            "Epoch 8/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 87ms/step - accuracy: 0.7739 - loss: 0.6465 - val_accuracy: 0.7184 - val_loss: 0.8613\n",
            "Epoch 9/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 89ms/step - accuracy: 0.7902 - loss: 0.5967 - val_accuracy: 0.7166 - val_loss: 0.8781\n",
            "Epoch 10/10\n",
            "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 92ms/step - accuracy: 0.8095 - loss: 0.5387 - val_accuracy: 0.7196 - val_loss: 0.8772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f17210d80e0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation.**\n"
      ],
      "metadata": {
        "id": "11VAhypqdZ6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Data Loaders\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Define CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3)      # 28 → 26\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)     # 26 → 24\n",
        "        self.pool = nn.MaxPool2d(2,2)         # 24 → 12\n",
        "\n",
        "        # 64 channels * 12 * 12 = 9216\n",
        "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "\n",
        "# Loss + Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(5):\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"Epoch:\", epoch+1, \"Loss:\", loss.item())\n",
        "\n",
        "# Evaluation\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(\"Test Accuracy:\", correct / total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOPIltpRd9ml",
        "outputId": "6a43bc2e-8b9b-4d4d-a5df-2ac23d737a31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 Loss: 0.01631917618215084\n",
            "Epoch: 2 Loss: 0.009268590249121189\n",
            "Epoch: 3 Loss: 0.0019121990771964192\n",
            "Epoch: 4 Loss: 0.0023090927861630917\n",
            "Epoch: 5 Loss: 0.029460176825523376\n",
            "Test Accuracy: 0.9876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model.**\n"
      ],
      "metadata": {
        "id": "QtlnqLdcdyND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1 — Upload ZIP file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# STEP 2 — Unzip\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "zip_name = list(uploaded.keys())[0]  # automatically gets the uploaded file name\n",
        "print(\"Uploaded file:\", zip_name)\n",
        "\n",
        "zip_ref = zipfile.ZipFile(io.BytesIO(uploaded[zip_name]), 'r')\n",
        "zip_ref.extractall('/content/rice')\n",
        "zip_ref.close()\n",
        "\n",
        "# Path to dataset\n",
        "data_path = \"/content/rice/Rice_Image_Dataset\"\n",
        "\n",
        "# STEP 3 — Image Preprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_data = datagen.flow_from_directory(\n",
        "    data_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = datagen.flow_from_directory(\n",
        "    data_path,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# STEP 4 — Build CNN Model\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(224,224,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(5, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# STEP 5 — Train\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    epochs=10,\n",
        "    validation_data=val_data\n",
        ")\n",
        "\n",
        "# STEP 6 — Save model in Keras 3 format\n",
        "model.save(\"rice_model.keras\")\n",
        "\n",
        "print(\"Model saved as rice_model.keras\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "nn3G_MVbi22U",
        "outputId": "0188c650-1041-4e56-dc1c-bc055bcdc9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-621feb8d-8e20-44ab-b52e-2fe0d8401728\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-621feb8d-8e20-44ab-b52e-2fe0d8401728\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10: You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit.**\n"
      ],
      "metadata": {
        "id": "49AOIuS-zfzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "**1. Problem framing**\n",
        "Binary classification: input = chest X-ray image, output = “Normal” or “Pneumonia”. Goal: high sensitivity for Pneumonia, robust generalisation, fast inference for web app.\n",
        "\n",
        "**2. Data preparation**\n",
        "\n",
        "Collect labeled X-rays, keep patient-level split so same patient isn’t in train+test.\n",
        "\n",
        "Inspect class balance and image sizes.\n",
        "\n",
        "Clean: remove corrupted files, consistent file formats, convert to single-channel (grayscale) or 3-channel if using ImageNet backbones.\n",
        "\n",
        "Split: train / val / test (e.g., 70/15/15) at patient-level.\n",
        "\n",
        "Augmentation (only for train): random rotation ±15°, horizontal flip (if clinically acceptable), random zoom, brightness/contrast jitter. Do NOT apply augmentation to validation/test.\n",
        "\n",
        "Normalize pixel values; if using pretrained backbone, use that model’s normalization (e.g., mean/std or scale to [0,1]).\n",
        "\n",
        "Handle imbalance: use class weights in loss, oversample minority, or focal loss.\n",
        "\n",
        "**3. Model choice & architecture**\n",
        "\n",
        "Prefer transfer learning (faster, better with limited medical data). Use a pretrained backbone (e.g., DenseNet121 / ResNet50 / EfficientNet) + small classification head: global average pooling → dropout → dense (1, sigmoid).\n",
        "\n",
        "If training from scratch, use a small custom CNN and heavy augmentation, but transfer learning is standard for X-rays.\n",
        "\n",
        "**4. Loss, metrics, and other training details**\n",
        "\n",
        "Loss: binary cross-entropy; consider focal loss if hard negatives or heavy imbalance.\n",
        "\n",
        "Metrics to monitor: sensitivity (recall) for Pneumonia, specificity, accuracy, AUC-ROC. Prioritize sensitivity in model selection.\n",
        "\n",
        "Optimizer: Adam with lr schedule; use ReduceLROnPlateau or cosine schedule.\n",
        "\n",
        "Regularization: dropout (0.3–0.5), weight decay.\n",
        "\n",
        "Early stopping on validation AUC/sensitivity.\n",
        "\n",
        "Use mixed precision and batch size tuning to speed training.\n",
        "\n",
        "Use stratified mini-batches and patient-level grouping when possible.\n",
        "\n",
        "**5. Training pipeline**\n",
        "\n",
        "Build data generators / tf.data pipeline for efficient IO and augmentation.\n",
        "\n",
        "Freeze backbone initially, train head for few epochs, then unfreeze some layers and fine-tune with a lower lr.\n",
        "\n",
        "Save best model checkpoint by validation AUC or sensitivity.\n",
        "\n",
        "Validate on unseen test set only once after final model chosen.\n",
        "\n",
        "**6. Model evaluation & validation**\n",
        "\n",
        "Evaluate test set for AUC, confusion matrix, sensitivity, specificity.\n",
        "\n",
        "Calibration: check reliability (e.g., calibration curve); apply temperature scaling if needed.\n",
        "\n",
        "Explainability: generate Grad-CAM / saliency maps for sample predictions to verify model focuses on lungs (important for clinical trust).\n",
        "\n",
        "Perform robustness checks: different image sources, scanners, noise.\n",
        "\n",
        "**7. Exporting & packaging model**\n",
        "\n",
        "Export final model to a format suitable for deployment: SavedModel (TensorFlow) or TorchScript/ONNX (PyTorch).\n",
        "\n",
        "Optionally create a lightweight version (quantized TFLite) for edge/mobile.\n",
        "\n",
        "Include preprocessing pipeline code with the model or bundle normalization parameters.\n",
        "\n",
        "**8. Web app design (Streamlit)**\n",
        "\n",
        "Frontend: simple UI to upload an X-ray, show original image + Grad-CAM overlay + predicted probability + label.\n",
        "\n",
        "Backend inference: load exported model once on app start, apply same preprocessing, run predict, optionally run explainability (Grad-CAM) and return results.\n",
        "\n",
        "Example minimal Streamlit flow: user uploads image → app preprocesses → model.predict → display probability and heatmap → allow download of report.\n",
        "\n",
        "\n",
        "**9. Deployment options**\n",
        "\n",
        "Local/Cloud: deploy Streamlit on a VM, or use Streamlit Cloud, Heroku, AWS EC2, or Docker + any cloud provider.\n",
        "\n",
        "Containerize: create Dockerfile including model files, expose Streamlit port.\n",
        "\n",
        "Scale: use Gunicorn + multiple workers or wrap model inference behind a REST microservice (FastAPI) and let frontend call API. Use GPU instances for high throughput.\n",
        "\n",
        "**10. Security, privacy & compliance**\n",
        "\n",
        "De-identify images and follow HIPAA/GDPR rules as applicable.\n",
        "\n",
        "Secure uploads (HTTPS), limit file sizes, validate image MIME types.\n",
        "\n",
        "Audit logs and user authentication for clinical use. Never store PHI unless permitted.\n",
        "\n",
        "**11. Monitoring & maintenance**\n",
        "\n",
        "Monitor latency, throughput, and model drift. Log predictions and periodically re-evaluate on new labeled data.\n",
        "\n",
        "Set up alerting if performance drops. Retrain with new data when performance degrades.\n",
        "\n",
        "**12. Testing & validation for clinical readiness**\n",
        "\n",
        "Extensive external validation on datasets from different hospitals.\n",
        "\n",
        "Clinical review of false positives/negatives.\n",
        "\n",
        "If intended for clinical use, follow regulatory requirements and perform prospective trials.\n",
        "\n",
        "**13. Checklist before release**\n",
        "\n",
        "Reproducible training script and fixed random seeds.\n",
        "\n",
        "Unit tests for preprocessing and inference.\n",
        "\n",
        "Key documentation: input format, expected normalization, model versioning, and instructions for rollback.\n",
        "\n",
        "**14. Quick summary (one-line)**\n",
        "Prepare patient-split, augmented data → use transfer learning and prioritize sensitivity → validate with AUC + Grad-CAM → export model → deploy as Streamlit app (Docker) with secure uploads, monitoring, and clinical validation.\n",
        "\n"
      ],
      "metadata": {
        "id": "wGyRp2jMk1n0"
      }
    }
  ]
}